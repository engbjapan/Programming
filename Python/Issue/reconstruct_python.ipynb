{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engbJapan/Programming/blob/main/Python/Issue/reconstruct_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jU2pgaIXkVme"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jieUk8gkVmn"
      },
      "source": [
        "\n",
        "# Reconstruct Python\n",
        "\n",
        "Demonstrates how Lark's experimental text-reconstruction feature can recreate\n",
        "functional Python code from its parse-tree, using just the correct grammar and\n",
        "a small formatter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e8G6IZnkkVmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "07925993-3c9c-4e56-8a5f-ec52f4742cca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0daf9641b706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-0daf9641b706>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     self_contents = open(\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;34mf\"os.getcwd()/gram\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         ).read()\n\u001b[1;32m     67\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_parser3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_contents\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'os.getcwd()/gram'"
          ]
        }
      ],
      "source": [
        "%pip install lark -qq\n",
        "from lark import Token, Lark\n",
        "from lark.reconstruct import Reconstructor\n",
        "from lark.indenter import PythonIndenter\n",
        "\n",
        "# Official Python grammar by Lark\n",
        "python_parser3 = Lark.open_from_package('lark', 'python.lark', ['grammars'],\n",
        "                                        parser='lalr', postlex=PythonIndenter(), start='file_input',\n",
        "                                        maybe_placeholders=False    # Necessary for reconstructor\n",
        "                                        )\n",
        "\n",
        "SPACE_AFTER = set(',+-*/~@<>=\"|:')\n",
        "SPACE_BEFORE = (SPACE_AFTER - set(',:')) | set('\\'')\n",
        "\n",
        "\n",
        "def special(sym):\n",
        "    return Token('SPECIAL', sym.name)\n",
        "\n",
        "def postproc(items):\n",
        "    stack = ['\\n']\n",
        "    actions = []\n",
        "    last_was_whitespace = True\n",
        "    for item in items:\n",
        "        if isinstance(item, Token) and item.type == 'SPECIAL':\n",
        "            actions.append(item.value)\n",
        "        else:\n",
        "            if actions:\n",
        "                assert actions[0] == '_NEWLINE' and '_NEWLINE' not in actions[1:], actions\n",
        "\n",
        "                for a in actions[1:]:\n",
        "                    if a == '_INDENT':\n",
        "                        stack.append(stack[-1] + ' ' * 4)\n",
        "                    else:\n",
        "                        assert a == '_DEDENT'\n",
        "                        stack.pop()\n",
        "                actions.clear()\n",
        "                yield stack[-1]\n",
        "                last_was_whitespace = True\n",
        "            if not last_was_whitespace:\n",
        "                if item[0] in SPACE_BEFORE:\n",
        "                    yield ' '\n",
        "            yield item\n",
        "            last_was_whitespace = item[-1].isspace()\n",
        "            if not last_was_whitespace:\n",
        "                if item[-1] in SPACE_AFTER:\n",
        "                    yield ' '\n",
        "                    last_was_whitespace = True\n",
        "    yield \"\\n\"\n",
        "\n",
        "\n",
        "class PythonReconstructor:\n",
        "    def __init__(self, parser):\n",
        "        self._recons = Reconstructor(parser, {'_NEWLINE': special, '_DEDENT': special, '_INDENT': special})\n",
        "\n",
        "    def reconstruct(self, tree):\n",
        "        return self._recons.reconstruct(tree, postproc)\n",
        "\n",
        "\n",
        "def test():\n",
        "    python_reconstructor = PythonReconstructor(python_parser3)\n",
        "\n",
        "#    self_contents = open(__file__).read()\n",
        "\n",
        "    self_contents = open(\n",
        "        f\"os.getcwd()/gram\"\n",
        "        ).read()\n",
        "    tree = python_parser3.parse(self_contents+'\\n')\n",
        "    output = python_reconstructor.reconstruct(tree)\n",
        "\n",
        "    tree_new = python_parser3.parse(output)\n",
        "    print(tree.pretty())\n",
        "    print(tree_new.pretty())\n",
        "    # assert tree.pretty() == tree_new.pretty()\n",
        "    assert tree == tree_new\n",
        "\n",
        "    print(output)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print('getcwd:      ', os.getcwd())\n",
        "#print('__file__:    ', __file__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZSN6-n5mQ8m",
        "outputId": "6450a094-cccd-4973-f82f-86ac5c1203bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getcwd:       /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lark import Lark\n",
        "\n",
        "grammar = r\"\"\"\n",
        "start: list | dict\n",
        "\n",
        "list: \"[\" _seperated{atom, \",\"} \"]\"\n",
        "dict: \"{\" _seperated{key_value, \",\"} \"}\"\n",
        "key_value: atom \":\" atom\n",
        "\n",
        "_seperated{x, sep}: x (sep x)*  // Define a sequence of 'x sep x sep x ...'\n",
        "\n",
        "atom: NUMBER | ESCAPED_STRING\n",
        "\n",
        "%import common (NUMBER, ESCAPED_STRING, WS)\n",
        "%ignore WS\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "parser = Lark(grammar)\n",
        "\n",
        "print(parser.parse('[1, \"a\", 2]'))\n",
        "print(parser.parse('{\"a\": 2, \"b\": 6}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHg4974wr6B-",
        "outputId": "c28ff2a6-1a7a-46da-baca-84600a731bf1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tree(Token('RULE', 'start'), [Tree(Token('RULE', 'list'), [Tree(Token('RULE', 'atom'), [Token('NUMBER', '1')]), Tree(Token('RULE', 'atom'), [Token('ESCAPED_STRING', '\"a\"')]), Tree(Token('RULE', 'atom'), [Token('NUMBER', '2')])])])\n",
            "Tree(Token('RULE', 'start'), [Tree(Token('RULE', 'dict'), [Tree(Token('RULE', 'key_value'), [Tree(Token('RULE', 'atom'), [Token('ESCAPED_STRING', '\"a\"')]), Tree(Token('RULE', 'atom'), [Token('NUMBER', '2')])]), Tree(Token('RULE', 'key_value'), [Tree(Token('RULE', 'atom'), [Token('ESCAPED_STRING', '\"b\"')]), Tree(Token('RULE', 'atom'), [Token('NUMBER', '6')])])])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<EOFFF > ./grammar.lark\n",
        "?start: statement+\n",
        " \n",
        "?statement: function\n",
        "    | instruction\n",
        "    | function_call\n",
        " \n",
        "code_block: \"{\" statement+ \"}\"\n",
        "function: \"関数\" new_symbol \"(\" \")\" code_block\n",
        "function_call: symbol \"(\" \")\"\n",
        "instruction: \"出力\" \"(\" string \")\" -> out\n",
        " \n",
        "string    : ESCAPED_STRING\n",
        "symbol    : WORD\n",
        "new_symbol: WORD\n",
        " \n",
        "%import common.ESCAPED_STRING\n",
        " \n",
        "%import common.WORD\n",
        " \n",
        "%import common.WS\n",
        " \n",
        "%ignore WS\n",
        "EOFFF\n",
        "cat ./grammar.lark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZjt-10rqSI-",
        "outputId": "80efa655-b835-4ac8-a345-59858da2a65c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?start: statement+\n",
            " \n",
            "?statement: function\n",
            "    | instruction\n",
            "    | function_call\n",
            " \n",
            "code_block: \"{\" statement+ \"}\"\n",
            "function: \"関数\" new_symbol \"(\" \")\" code_block\n",
            "function_call: symbol \"(\" \")\"\n",
            "instruction: \"出力\" \"(\" string \")\" -> out\n",
            " \n",
            "string    : ESCAPED_STRING\n",
            "symbol    : WORD\n",
            "new_symbol: WORD\n",
            " \n",
            "%import common.ESCAPED_STRING\n",
            " \n",
            "%import common.WORD\n",
            " \n",
            "%import common.WS\n",
            " \n",
            "%ignore WS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from lark import Lark, Transformer\n",
        " \n",
        "def out(string):\n",
        "    print(string[0])\n",
        " \n",
        "class Main(Transformer):\n",
        "    def __init__(self):\n",
        "        self._functions = {}\n",
        " \n",
        "    def function(self, token):\n",
        "        self._functions[token[0]] = token[1:]\n",
        "    \n",
        "    def function_call(self, token):\n",
        "        function = self._functions[token[0]]\n",
        "        for state in function:\n",
        "            eval(state.data)(state.children)\n",
        " \n",
        "    def code_block(self, tree):\n",
        "        return tree[0]\n",
        " \n",
        "    def new_symbol(self, token):\n",
        "        return token[0].value\n",
        "    \n",
        "    def symbol(self, token):\n",
        "        return token[0].value\n",
        " \n",
        "    def string(self, token):\n",
        "        return token[0][1:-1]\n",
        " \n",
        "text = '''\n",
        "    関数 main(){\n",
        "        出力(\"Hello world\")\n",
        "    }\n",
        "    main()\n",
        "    print(\"dosita?\")\n",
        "    '''\n",
        " \n",
        "# ファイル分けしたので、中身全部読んで変数に入れる\n",
        "grammar = \"\"\n",
        "with open('./grammar.lark', 'r', encoding='utf-8') as a_file:\n",
        "    grammar = ''.join([line for line in a_file])\n",
        " \n",
        "parser = Lark(grammar, parser='lalr', transformer=Main())\n",
        "parser.parse(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "R_3zBhkfpU-p",
        "outputId": "2f103dd7-0982-45fb-f1cd-b7bf64eebf71"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello world\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnexpectedToken",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnexpectedCharacters\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lexer.py\u001b[0m in \u001b[0;36mlex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparser_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lexer.py\u001b[0m in \u001b[0;36mnext_token\u001b[0;34m(self, lex_state, parser_state)\u001b[0m\n\u001b[1;32m    467\u001b[0m                                            \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlex_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlex_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                                            state=parser_state, terminals_by_name=self.terminals_by_name)\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnexpectedCharacters\u001b[0m: No terminal matches '\"' in the current parser context, at line 6 col 11\n\n    print(\"dosita?\")\n          ^\nExpected one of: \n\t* RPAR\n\nPrevious tokens: Token('LPAR', '(')\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnexpectedToken\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d5ca3a678de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lalr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lark.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parser_frontends.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'on_error'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_lexer_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, lexer, start, on_error)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnexpectedInput\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_interactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mInteractiveParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_from_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse_from_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse_from_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lexer.py\u001b[0m in \u001b[0;36mlex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mlast_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m  \u001b[0;31m# Save last_token. Calling root_lexer.next_token will change this to the wrong token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_lexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnexpectedToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals_by_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_lexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals_by_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnexpectedCharacters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# Raise the original UnexpectedCharacters. The root lexer raises it with the wrong expected set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnexpectedToken\u001b[0m: Unexpected token Token('ESCAPED_STRING', '\"dosita?\"') at line 6, column 11.\nExpected one of: \n\t* RPAR\nPrevious tokens: [Token('LPAR', '(')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}