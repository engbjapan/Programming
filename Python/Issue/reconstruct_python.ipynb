{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/engbJapan/Programming/blob/main/Python/Issue/reconstruct_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#構文解析の拡張がBNF(EBNF）定義で可能だったんか。"
      ],
      "metadata": {
        "id": "_P3t-HZ6ohBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lark!!\n",
        "\n",
        "https://lark-parser.readthedocs.io/en/latest/examples/index.html\n",
        "\n",
        "https://github.com/lark-parser/lark\n",
        "\n",
        "\n",
        "REF\n",
        "\n",
        "https://developers.10antz.co.jp/archives/2007\n",
        "\n",
        "https://zenn.dev/ookuwab/articles/9137f640a88bd8#%E5%8F%82%E8%80%83%E3%81%AB%E3%81%97%E3%81%9F%E3%82%B5%E3%82%A4%E3%83%88\n",
        "\n",
        "https://blog.panicblanket.com/archives/6279\n",
        "\n",
        "https://qiita.com/k5trismegistus/items/358d58bcd50eb3f67fe8\n",
        "\n",
        "\n",
        "PythonGrammer\n",
        "\n",
        "https://docs.python.org/ja/3.7/reference/grammar.html\n"
      ],
      "metadata": {
        "id": "5J0oihwhnAR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jU2pgaIXkVme"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jieUk8gkVmn"
      },
      "source": [
        "\n",
        "# Reconstruct Python\n",
        "\n",
        "Demonstrates how Lark's experimental text-reconstruction feature can recreate\n",
        "functional Python code from its parse-tree, using just the correct grammar and\n",
        "a small formatter.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install lark -qq\n",
        "from lark import Lark\n",
        "\n",
        "#ここらをモディファイ\n",
        "grammar = r\"\"\"\n",
        "start: list | dict\n",
        "\n",
        "list: \"[\" _seperated{atom, \",\"} \"]\"\n",
        "dict: \"{\" _seperated{key_value, \",\"} \"}\"\n",
        "key_value: atom \":\" atom\n",
        "\n",
        "_seperated{x, sep}: x (sep x)*  // Define a sequence of 'x sep x sep x ...'\n",
        "\n",
        "atom: NUMBER | ESCAPED_STRING\n",
        "\n",
        "%import common (NUMBER, ESCAPED_STRING, WS)\n",
        "%ignore WS\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "parser = Lark(grammar)\n",
        "\n",
        "#ここも、構文木を見やすくするためParseオブジェクトにprettyメソッドを追加して出すようにモディファイ\n",
        "print(parser.parse('[1, \"a\", 2]').pretty())\n",
        "print(parser.parse('{\"a\": 2, \"b\": 6}').pretty())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHg4974wr6B-",
        "outputId": "03a4463b-a39c-406f-dde5-aac149c5f7df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "  list\n",
            "    atom\t1\n",
            "    atom\t\"a\"\n",
            "    atom\t2\n",
            "\n",
            "start\n",
            "  dict\n",
            "    key_value\n",
            "      atom\t\"a\"\n",
            "      atom\t2\n",
            "    key_value\n",
            "      atom\t\"b\"\n",
            "      atom\t6\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ref1\n",
        "https://developers.10antz.co.jp/archives/2007\n",
        "\n",
        "functionの日本語化の実行例\n",
        "（拡張ではなく、この宣言内のみの対応になっとる）"
      ],
      "metadata": {
        "id": "0pBN_bFToILU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "そして実行し、動く"
      ],
      "metadata": {
        "id": "DR73PtoepTqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install lark -qq\n",
        "from lark import Lark, Transformer\n",
        " \n",
        "def out(string):\n",
        "    print(string[0])\n",
        " \n",
        "class Main(Transformer):\n",
        "    def __init__(self):\n",
        "        self._functions = {}\n",
        " \n",
        "    def function(self, token):\n",
        "        self._functions[token[0]] = token[1:]\n",
        "    \n",
        "    def function_call(self, token):\n",
        "        function = self._functions[token[0]]\n",
        "        for state in function:\n",
        "            eval(state.data)(state.children)\n",
        " \n",
        "    def code_block(self, tree):\n",
        "        return tree[0]\n",
        " \n",
        "    def new_symbol(self, token):\n",
        "        return token[0].value\n",
        "    \n",
        "    def symbol(self, token):\n",
        "        return token[0].value\n",
        " \n",
        "    def string(self, token):\n",
        "        return token[0][1:-1]\n",
        " \n",
        " \n",
        "# グラマー定義（EBNF記法での）を変数に紐づけ\n",
        "# function キーワードを関数として、Cライクなブロック記法を導入した物\n",
        "# 参照元の手法は外部ファイルとして居るところをダイレクトに変数に三重引用符書き綴り紐づけた\n",
        "grammar = '''\n",
        "?start: statement+\n",
        " \n",
        "?statement: function\n",
        "    | instruction\n",
        "    | function_call\n",
        " \n",
        "code_block: \"{\" statement+ \"}\"\n",
        "function: \"関数\" new_symbol \"(\" \")\" code_block\n",
        "function_call: symbol \"(\" \")\"\n",
        "instruction: \"出力\" \"(\" string \")\" -> out\n",
        " \n",
        "string    : ESCAPED_STRING\n",
        "symbol    : WORD\n",
        "new_symbol: WORD\n",
        " \n",
        "%import common.ESCAPED_STRING\n",
        " \n",
        "%import common.WORD\n",
        " \n",
        "%import common.WS\n",
        " \n",
        "%ignore WS\n",
        "'''\n",
        "# 参照元の手法は外部ファイルとして居るところをダイレクトに変数にfリテラルで紐づけた\n",
        "#ので以下のファイルからのリードやらの面倒な処理削除\n",
        "#with open('./grammar.lark', 'r', encoding='utf-8') as a_file:\n",
        "#    grammar = ''.join([line for line in a_file])\n",
        " \n",
        "parser = Lark(grammar, parser='lalr', transformer=Main())\n",
        "# 参照元の実行コードをこれまた、三重引用符で書き綴り与えた\n",
        "parser.parse('''\n",
        "関数 main(){\n",
        "        出力(\"ようこそ❢\")\n",
        "}\n",
        "main()\n",
        "'''\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3zBhkfpU-p",
        "outputId": "83a76fbb-9f33-4931-f245-292fdc88d3b2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ようこそ❢\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree(Token('RULE', 'start'), [None, None])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e8G6IZnkkVmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d84a019e-2e60-4a81-d01f-03cf5f4803f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnexpectedToken",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnexpectedCharacters\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lexer.py\u001b[0m in \u001b[0;36mlex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mlexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparser_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lexer.py\u001b[0m in \u001b[0;36mnext_token\u001b[0;34m(self, lex_state, parser_state)\u001b[0m\n\u001b[1;32m    467\u001b[0m                                            \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlex_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlex_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                                            state=parser_state, terminals_by_name=self.terminals_by_name)\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnexpectedCharacters\u001b[0m: No terminal matches 'n' in the current parser context, at line 3 col 16\n\nfunction: \"関数\" new_symbol \"(\" \")\" code_block\n               ^\nExpected one of: \n\t* MINUS\n\t* \"/=\"\n\t* \"<=\"\n\t* FROM\n\t* AS\n\t* \"@=\"\n\t* \"-=\"\n\t* SLASH\n\t* RBRACE\n\t* \"+=\"\n\t* OR\n\t* \"==\"\n\t* \">=\"\n\t* NOT\n\t* \"<<\"\n\t* IN\n\t* AMPERSAND\n\t* COMMA\n\t* \"<<=\"\n\t* AND\n\t* RPAR\n\t* FOR\n\t* ELSE\n\t* ASYNC\n\t* \">>\"\n\t* \"<>\"\n\t* RSQB\n\t* PLUS\n\t* CIRCUMFLEX\n\t* STAR\n\t* STRING\n\t* LESSTHAN\n\t* AT\n\t* \"%=\"\n\t* \"*=\"\n\t* \"^=\"\n\t* \"//=\"\n\t* IS\n\t* DOT\n\t* LSQB\n\t* PERCENT\n\t* SEMICOLON\n\t* MORETHAN\n\t* VBAR\n\t* \"!=\"\n\t* LONG_STRING\n\t* COLON\n\t* IF\n\t* EQUAL\n\t* \"**=\"\n\t* \"&=\"\n\t* _NEWLINE\n\t* \">>=\"\n\t* \"**\"\n\t* LPAR\n\t* \"|=\"\n\t* \"//\"\n\nPrevious tokens: Token('STRING', '\"関数\"')\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnexpectedToken\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a5189c150756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-a5189c150756>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m %ignore WS'''\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m#    tree = python_parser3.parse(self_contents+'\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_parser3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_contents\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_reconstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lark.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parser_frontends.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'on_error'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_lexer_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, lexer, start, on_error)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnexpectedInput\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_interactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mInteractiveParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_from_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse_from_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/parsers/lalr_parser.py\u001b[0m in \u001b[0;36mparse_from_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/indenter.py\u001b[0m in \u001b[0;36m_process\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNL_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_NL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lark/lexer.py\u001b[0m in \u001b[0;36mlex\u001b[0;34m(self, lexer_state, parser_state)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mlast_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexer_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m  \u001b[0;31m# Save last_token. Calling root_lexer.next_token will change this to the wrong token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_lexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mUnexpectedToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals_by_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_lexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals_by_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnexpectedCharacters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# Raise the original UnexpectedCharacters. The root lexer raises it with the wrong expected set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnexpectedToken\u001b[0m: Unexpected token Token('NAME', 'new_symbol') at line 3, column 16.\nExpected one of: \n\t* MINUS\n\t* \"<=\"\n\t* SLASH\n\t* \"==\"\n\t* OR\n\t* \">=\"\n\t* NOT\n\t* \"<<\"\n\t* IN\n\t* AMPERSAND\n\t* AND\n\t* \">>\"\n\t* \"<>\"\n\t* PLUS\n\t* CIRCUMFLEX\n\t* STAR\n\t* STRING\n\t* LESSTHAN\n\t* AT\n\t* IS\n\t* DOT\n\t* LSQB\n\t* PERCENT\n\t* VBAR\n\t* SEMICOLON\n\t* MORETHAN\n\t* \"!=\"\n\t* IF\n\t* EQUAL\n\t* _NEWLINE\n\t* LONG_STRING\n\t* \"**\"\n\t* LPAR\n\t* \"//\"\nPrevious tokens: [Token('STRING', '\"関数\"')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from lark import Token, Lark\n",
        "from lark.reconstruct import Reconstructor\n",
        "from lark.indenter import PythonIndenter\n",
        "\n",
        "# Official Python grammar by Lark\n",
        "python_parser3 = Lark.open_from_package('lark', 'python.lark', ['grammars'],\n",
        "                                        parser='lalr', postlex=PythonIndenter(), start='file_input',\n",
        "                                        maybe_placeholders=False    # Necessary for reconstructor\n",
        "                                        )\n",
        "\n",
        "SPACE_AFTER = set(',+-*/~@<>=\"|:')\n",
        "SPACE_BEFORE = (SPACE_AFTER - set(',:')) | set('\\'')\n",
        "\n",
        "\n",
        "def special(sym):\n",
        "    return Token('SPECIAL', sym.name)\n",
        "\n",
        "def postproc(items):\n",
        "    stack = ['\\n']\n",
        "    actions = []\n",
        "    last_was_whitespace = True\n",
        "    for item in items:\n",
        "        if isinstance(item, Token) and item.type == 'SPECIAL':\n",
        "            actions.append(item.value)\n",
        "        else:\n",
        "            if actions:\n",
        "                assert actions[0] == '_NEWLINE' and '_NEWLINE' not in actions[1:], actions\n",
        "\n",
        "                for a in actions[1:]:\n",
        "                    if a == '_INDENT':\n",
        "                        stack.append(stack[-1] + ' ' * 4)\n",
        "                    else:\n",
        "                        assert a == '_DEDENT'\n",
        "                        stack.pop()\n",
        "                actions.clear()\n",
        "                yield stack[-1]\n",
        "                last_was_whitespace = True\n",
        "            if not last_was_whitespace:\n",
        "                if item[0] in SPACE_BEFORE:\n",
        "                    yield ' '\n",
        "            yield item\n",
        "            last_was_whitespace = item[-1].isspace()\n",
        "            if not last_was_whitespace:\n",
        "                if item[-1] in SPACE_AFTER:\n",
        "                    yield ' '\n",
        "                    last_was_whitespace = True\n",
        "    yield \"\\n\"\n",
        "\n",
        "\n",
        "class PythonReconstructor:\n",
        "    def __init__(self, parser):\n",
        "        self._recons = Reconstructor(parser, {'_NEWLINE': special, '_DEDENT': special, '_INDENT': special})\n",
        "\n",
        "    def reconstruct(self, tree):\n",
        "        return self._recons.reconstruct(tree, postproc)\n",
        "\n",
        "\n",
        "def test():\n",
        "    python_reconstructor = PythonReconstructor(python_parser3)\n",
        "\n",
        "#    self_contents = open(__file__).read()\n",
        "\n",
        "    self_contents = f'''statement: function| instruction| function_call\n",
        "code_block: \"{\" statement+ \"}\"\n",
        "function: \"関数\" new_symbol \"(\" \")\" code_block\n",
        "function_call: symbol \"(\" \")\"\n",
        "instruction: \"出力\" \"(\" string \")\" -> out\n",
        " \n",
        "string    : ESCAPED_STRING\n",
        "symbol    : WORD\n",
        "new_symbol: WORD\n",
        " \n",
        "%import common.ESCAPED_STRING\n",
        " \n",
        "%import common.WORD\n",
        " \n",
        "%import common.WS\n",
        " \n",
        "%ignore WS'''\n",
        "    #    tree = python_parser3.parse(self_contents+'\\n')\n",
        "    tree = python_parser3.parse(self_contents+'\\n')\n",
        "    output = python_reconstructor.reconstruct(tree)\n",
        "\n",
        "    tree_new = python_parser3.parse(output)\n",
        "    print(tree.pretty())\n",
        "    print(tree_new.pretty())\n",
        "    # assert tree.pretty() == tree_new.pretty()\n",
        "    assert tree == tree_new\n",
        "\n",
        "    print(output)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}